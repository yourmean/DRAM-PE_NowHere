{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*해당 자료는 김성범 교수님의 [핵심 머신러닝] Decision Tree 1, 2를 참고했음을 밝힙니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터에 내재되어 있는 패턴을 변수의 조합으로 나타내는 예측/분류 모델을 나무의 형태로 만드는 것\n",
    "* if-then-else 결정 규칙을 통해 데이터 학습\n",
    "* 트리의 깊이가 깊을수록 복잡한 모델\n",
    "* \"스무고개\" 놀이와 비슷한 개념"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_001.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측나무모델(Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_002.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Tree Model : f(x)\n",
    "* Training Parameter : c(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_003.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_004.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모든 변수와 모든 점의 조합에 대하여 분할이 되었을 때의 각각의 cost function을 구해, 그 값이 최소가 되는 분할 변수(j)와 분할점(s)을 구한다. # Greedy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류나무모델(Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_005.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_006.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_007.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류나무 모델링 프로세스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류모델의 분할법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분할 변수(j)와 분할 기준(s)은 목표 변수(y)의 분포를 가장 잘 구별해주는 쪽으로 정함\n",
    "* 목표 변수의 분포를 잘 구별해주는 측도로 순수도(purity) 또는 불순도(impurity)를 정의\n",
    "* 예를 들어 클래스 0과 클래스 1의 비율이 45%와 55%인 노드는 각 클래스의 비율이 90%와 10%인 마디에 비하여 순수도가 낮다(또는 불순도가 높다)라고 해석\n",
    "* 각 노드에서 분할 변수와 분할점의 설정은 불순도의 감소가 최대가 되도록 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_008.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분류모델에서는 y가 범주이기 때문에 Cost Function으로 (y - y_hat)^2 를 정의하기가 어렵기 때문에 새로운 접근 방식 필요\n",
    "* 상기 이미지에서 Cross-Entropy는 scale된 graph (range: 0 ~ 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오분류율(misclassification rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_009.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_010.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Image/DT_011.PNG' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모든 변수에 대하여 분할이 되었을 때의 각각의 cost function을 구해, 전체 Cost Function 대비 gain이 최대가 되는(entropy 감소량이 최대가 되는) 분할 변수(j)와 분할점(s)을 구한다. # Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이해와 해석이 쉽다(판단/주장에 설득력을 얻는다).\n",
    "* 시각화가 용이하다.\n",
    "* 많은 데이터 전처리가 필요하지 않다(규칙을 학습하기 때문에 전처리에 큰 영향을 받지 않는다).\n",
    "* 수치형과 범주형 데이터 모두를 다룰 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 계층적 구조로 인해 중간에 에러가 발생하면 다음 단계로 에러가 계속 전파\n",
    "* 학습 데이터의 미세한 변동에도 최종 결과 크게 영향\n",
    "* 적은 개수의 노이즈에도 크게 영향\n",
    "* 나무의 최종 노드 개수를 늘리면 과적합 위험(Low Bias, Large Variance)\n",
    "* 해결 방안 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Regressor**\n",
    "<br>\n",
    "<br>\n",
    "* 비슷한 수치를 갖고 있는 관측치끼리 모음\n",
    "* cost function : (y - y_hat)^2\n",
    "* 모든 변수와 모든 점의 조합에 대하여 분할이 되었을 때의 각각의 cost function을 구해, 그 값이 최소가 되는 분할 변수(j)와 분할점(s)을 구한다. # Greedy Search\n",
    "* 각 분할에 속해 있는 y값들의 평균으로 예측했을 때 오류가 최소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**\n",
    "<br>\n",
    "<br>\n",
    "* 비슷한 범주를 갖고 있는 관측치끼리 모음\n",
    "* cost function: misclassification rate(0 ~ 0.5), Gini(0 ~ 0.5), Entropy(0 ~ 1)\n",
    "* 모든 변수에 대하여 분할이 되었을 때의 각각의 cost function을 구해, 전체 Cost Function 대비 gain이 최대가 되는(entropy 감소량이 최대가 되는) 분할 변수(j)와 분할점(s)을 구한다. # Information Gain\n",
    "* 각 분할에 속해 있는 k(m) 클래스로 예측했을 때 오류가 최소 (k(m): m 노드에서 portion이 가장 많은 class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
