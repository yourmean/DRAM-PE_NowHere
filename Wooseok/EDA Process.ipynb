{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-lj8Q3o16qP"
   },
   "source": [
    "#EDA Process\n",
    "EDA : Exploratory Data Analysis 탐색적 데이터 분석\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "1. 시각화를 통한 분포 확인\n",
    "  - histogram, pie-chart, boxplot, qqplot(분포) 등\n",
    "  \n",
    "2. Summary Statistics : 데이터 분포 및 특이성 확인\n",
    "  -  mean, sd, skewness, kurtosis, outliers, frequency 등\n",
    "  \n",
    "3. 독립변수 선정\n",
    "  - 데이터 해석 상의 배경지식 반영\n",
    "  - 상관성 높은 변수 제거 (비추)\n",
    "  - 차원 축소 (설명력 높은 수준의 PCA)\n",
    "  - 파생변수 생성\n",
    "  \n",
    "4. Data Filtering\n",
    "  - Data type 확인(수치형 <> 범주형)\n",
    "  - 중복 데이터 확인\n",
    "  - 결측치 제거 및 중위값 대체\n",
    "  - outlier 처리\n",
    "  - 범주형 : one-hot encoding\n",
    "  \n",
    "5. 범주형 변수 처리\n",
    "  \n",
    "6. scaling\n",
    "  - z-normalize, MinMax\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "splLRGr716Lo"
   },
   "source": [
    "1. 시각화를 통한 분포 확인\n",
    "  - histogram, pie-chart, boxplot, qqplot(분포) 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수들의 분포 시각적 확인\n",
    "f,ax=plt.subplots(4,3,figsize=(19,6),constrained_layout = True)\n",
    "\n",
    "sns.distplot(df[\"A\"],bins=20,ax=ax[0,0],color='orange');\n",
    "sns.distplot(df[\"B\"],bins=20,ax=ax[0,1],color='orange');\n",
    "sns.distplot(df[\"C\"],bins=20,ax=ax[0,2],color='orange');\n",
    "sns.distplot(df[\"D\"],bins=20,ax=ax[1,0],color='red');\n",
    "sns.distplot(df[\"E\"],bins=20,ax=ax[1,1],color='red');\n",
    "sns.distplot(df[\"F\"],bins=20,ax=ax[1,2],color='red');\n",
    "sns.distplot(df[\"G\"],bins=20,ax=ax[2,0],color='black');\n",
    "sns.distplot(df[\"H\"],bins=20,ax=ax[2,1],color='black');\n",
    "sns.distplot(df[\"I\"],bins=20,ax=ax[2,2],color='black');\n",
    "sns.distplot(df[\"J\"],bins=20,ax=ax[3,0],color='gray');\n",
    "sns.distplot(df[\"K\"],bins=20,ax=ax[3,1],color='gray');\n",
    "sns.distplot(df[\"L\"],bins=20,ax=ax[3,2],color='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/galipsekeroglu/randomforest-explainableai-xai-eda-gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립변수 A에 따른 target 변수(price) 분포 : Skewness 확인\n",
    "plt.figure(figsize=(20,8))\n",
    "g = sns.scatterplot(x='A',y='Price',data=df)\n",
    "\n",
    "g.set_title('A VS Price Correlation',fontsize=20)\n",
    "g.set_xlabel('Ar',fontsize=12)\n",
    "g.set_ylabel('Price',fontsize=12)\n",
    "\n",
    "xlabels = ['{:,.2f}'.format(x)+'k' for x in g.get_xticks()/10e3]\n",
    "ylabels = ['{:,.2f}'.format(y)+'k' for y in g.get_yticks()/10e3]\n",
    "\n",
    "g.set_xticklabels(xlabels)\n",
    "g.set_yticklabels(ylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V57McMy08SWz"
   },
   "source": [
    "2. Summary Statistics : 데이터 분포 및 특이성 확인\n",
    "  -  mean, sd, skewness, kurtosis, outliers, frequency 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mm4nJjTT89fW"
   },
   "outputs": [],
   "source": [
    "df.head(n) # DataFrame 형태로 데이터 상위 n개 확인\n",
    "\n",
    "df.info() # class, Column별 데이터 갯수 및 type 확인\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.naver.com/charzim0611/222922894275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1FXL-2E8SQE"
   },
   "source": [
    "3. 독립변수 선정\n",
    "  - 데이터 해석 상의 배경지식 반영\n",
    "  - 상관성 높은 변수 제거 (비추)\n",
    "  - 차원 축소 (설명력 높은 수준의 PCA)\n",
    "  - 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 변수들간의 상관관계 파악, 시각화 -> heatmap\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(df.corr(), annot=True, vmax=0.9, square=True,cmap='RdYlGn') # 변수간 상관관계 corr()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lOqbzUnCeeG"
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 0.9 이상의 강한 상관관계가 있는 변수들(A~Z)을 선형변환을 통해 차원 축소\n",
    "col_PCA = ['A,B...,Z']\n",
    "x = df[col_PCA]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "scaled_X_train = scaler.transform(x)\n",
    "\n",
    "pca = PCA(n_components=2) #PCA 객체 생성 (주성분 갯수 2개 생성)\n",
    "pca.fit(scaled_X_train)\n",
    "principalComponents = pca.transform(scaled_X_train)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['PCA_1', 'PCA_2'])\n",
    "principalDf\n",
    "\n",
    "pca.explained_variance_ratio_  # PCA1,PCA2 2개의 성분으로 cover하고 있는 설명력 확인\n",
    "\n",
    "# 두 PCA1,PCA2 변수를 추가하고 사용한 변수는 제거한다.\n",
    "df.insert(0,\"PCA_2\",principalDf.PCA_2,True) \n",
    "df.insert(0,\"PCA_1\",principalDf.PCA_1,True)\n",
    "df = df.drop(col_PCA,axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파생변수 생성(예)\n",
    "\n",
    "#년월, 일 column을 하나의 datetime변수로 합치기.\n",
    "yr=[]\n",
    "for i,j in np.array(df[['년월','일']]): #년월 : 202302 / 일 : 28\n",
    "    yr.append(datetime.datetime(i//100, i%100, j))\n",
    "df['년월일']=yr_date\n",
    "\n",
    "df['new_col'] = df['A'].str.replace(pat='.',repl='_') # 특정 col의 패턴 및 글자를 대체 (.->_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbdBY8aA8SER"
   },
   "source": [
    "4. Data Filtering\n",
    "  - Data type 변환 (수치형 <> 범주형)\n",
    "  - 중복 데이터 확인\n",
    "  - 결측치 제거 및 중위값 대체\n",
    "  - outlier 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 확인\n",
    "df.dtypes\n",
    "\n",
    "# 데이터타입 변환 (특정 column을 float으로)\n",
    "df.astype({'column' : 'float'}) # 방법 1\n",
    "df['A'] = df['A'].astype('int') # 방법 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ymd3rDJyCezI"
   },
   "outputs": [],
   "source": [
    "# 1. null값들의 존재 유무 및 그 비율 확인\n",
    "for column in df.columns:\n",
    "    if df[column].isna().sum()!=0:\n",
    "        missing = df[column].isna().sum()\n",
    "        portion = (missing/df.shape[0])*100\n",
    "        print(f\"'{column}':number of missing value '{missing}' ==> '{portion:.3f}%'\")\n",
    "        if portion >= 0.5:  # 결측치가 일정 비율 이상이면 변수 삭제 (비추)\n",
    "            df.drop(columns = column, axis=1) \n",
    "            print(f\"'{column}'is eliminated by portion'\")\n",
    "    print('==========================================')\n",
    "    \n",
    "# 2. column마다의 null 갯수\n",
    "data.isnull().sum()\n",
    "\n",
    "# 3. null 유무 확인 (T/F)\n",
    "df.isnull().values.any()\n",
    "\n",
    "## null 비율이 큰 변수는 제거하고 작은 변수는 최빈값(mode)으로 대체\n",
    "mode = df['A'].value_counts().index[0]\n",
    "df['A'] = df['A'].fillna(mode)\n",
    "df['A'] = df['A'].replace('-',mode)\n",
    "\n",
    "## null인 값을 정교하게 예측해서 값을 대체할 수도 있음 <자료 참고>\n",
    "# 방법 1: M/L RF로 특정 변수의 null값들을 대체 <ADX-Feature Engineering~>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방법 2 : MissingForest **(참고용)** <ADX-P1~>\n",
    "!pip install missingpy\n",
    "\n",
    "#아직 NaN값이 존재하는 column들 확인\n",
    "for i in df.columns:\n",
    "  if df[i].isnull().sum() != 0:\n",
    "    \n",
    "    print(i)\n",
    "    print(df[i].isnull().sum())\n",
    "    \n",
    "y_temp = df['SUCCPRIC']    \n",
    "    \n",
    "##MissingForest 사용하여 나머지 결측&이상치 대체\n",
    "from missingpy import MissForest\n",
    "\n",
    "# Make an instance and perform the imputation\n",
    "imputer = MissForest()\n",
    "df_temp = df.drop('SUCCPRIC',axis=1)  # target값 제거 후 예측. 후에 validation parting 나눌 때 필요하므로 다시 concat 할 예정\n",
    "imputer.fit(df_temp)  # fit처리를 저장 하여, 후에 test데이터에 transform 해주기 위함. imputer.transform(df_test)하면 가능\n",
    "X_imputer = imputer.transform(df_temp)\n",
    "X_imputer = pd.DataFrame(X_imputer, columns=df_temp.columns.tolist())\n",
    "\n",
    "# 아직 null이 존재하는지 재확인 후 index 초기화\n",
    "X_imputer.isnull().sum().sum()\n",
    "X_imputer.reset_index().drop('index', axis = 1)\n",
    "\n",
    "##df_temp 과 df['SUCCPRIC'] concat -> 다시 원래 데이터 모형으로\n",
    "y_temp = df['SUCCPRIC']\n",
    "y_temp = pd.DataFrame(y_temp).reset_index().drop('index', axis = 1)\n",
    "X_MF = pd.concat([X_imputer,y_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9t4itnZD0p2"
   },
   "outputs": [],
   "source": [
    "# 중복되는 데이터가 존재하는 확인\n",
    "print(f\"Data shape:{df.shape}\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Data shape:{df.shape}\")\n",
    "print('==========================================')\n",
    "\n",
    "# 중복 유무 확인\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Q8ivhE_D0mk"
   },
   "outputs": [],
   "source": [
    "# 1%의 이상치 제거\n",
    "df = df.loc[np.where(df['A'] < df['A'].quantile(0.99))] # 변수A에 대해선 큰값 1% 제거\n",
    "df.reset_index(drop=True, inplace= True)\n",
    "\n",
    "df = df.loc[np.where(df['B'] > df['B'].quantile(0.01))]  # 변수B에 대해선 작은 값 1% 제거\n",
    "df.reset_index(drop=True, inplace= True)\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYBexMQ1CfOp"
   },
   "source": [
    "5. 범주형 변수 : One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#범주형 변수 모음 A, 각 변수별 갯수(분포) 확인\n",
    "sns.countplot(x = 'A', data = df)\n",
    "\n",
    "# 특정 범주형 변수(subgrade)의 종류에 따라 종속변수(categorical)가 어떤 portion을 가니는지 확인 \n",
    "for subgrades in np.sort(df.sub_grade.unique()):\n",
    "    print(f\"{subgrades} subgrade in this position:\")\n",
    "    print(f\"{df[df.sub_grade == subgrades].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features 파악\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object:\n",
    "        print(column)\n",
    "        print(df[column].unique())\n",
    "        print(\"\")\n",
    "\n",
    "# one-hot encoding\n",
    "onehot = pd.get_dummies(df[CATEGORICAL], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Category마다의 종속변수에 대한 평균값, 분포를 확인 \n",
    "for j in CATEGORICAL:\n",
    "    y=[]\n",
    "    for i in df[j].unique():\n",
    "        y.append(df[df[j]==i]['가격'].values)\n",
    "      \n",
    "    fig, ax = plt.subplots()\n",
    "    bp = ax.boxplot(y, patch_artist=True)\n",
    "    ax.set_xticklabels(df[j].unique())\n",
    "    ax.set_title('가격')\n",
    "    ax.set_ylabel('가격')\n",
    "    ax.set_xlabel(j)\n",
    "    \n",
    "    print(df.groupby(df[j])['가격'].describe()[['count','mean','std']].sort_values(by='mean',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Scaling\n",
    "  \n",
    "모든 변수 전처리 완료가 되었다면 전체 scaling 후에 모델을 학습시켜야\n",
    "특정 변수에 의한 왜곡 발생 위험을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 스케일링 함수\n",
    "def z_normalize (train, list): # 대상이 될 dataset과 feature list를 받으면 normalization 해주는 함수. 이때 사용된 평균과 표준편차를 함께 return한다.\n",
    "    data=train.loc[:, list]\n",
    "    mean=data.mean()  # mean 저장\n",
    "    std=data.std()    # standard deviation 저장\n",
    "    data=(data-mean)/std  # normalization\n",
    "    return data, [mean,std]\n",
    "\n",
    "def minmax_scale (train, list):\n",
    "    data=train.loc[:, list]\n",
    "    min=data.min()    # min 저장\n",
    "    max=data.max()    # max 저장\n",
    "    data=(data-min)/(max-min)  # scale\n",
    "    return data, [min,max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWZMEuYC8jEL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42SGUkdB1vI9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVpKG6P_1zPb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp3OZDiL1zRj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtjlHXSU1zTy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh2CXCPe1zWv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6DT29gK1zaA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
